{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classical Control\n",
    "## Note the controller's output is disrete which complicated the original intention of using classical PID Control\n",
    "\n",
    "## HyperParameters\n",
    "render = True\n",
    "\n",
    "# https://gym.openai.com/docs\n",
    "\n",
    "# Guesses at observations are: (https://gym.openai.com/evaluations/eval_VQwN8kRESjakUPwJbRlq5Q)\n",
    "# observation[0] = pole speed\n",
    "# observation[1] = pole top pos\n",
    "# observation[2] = pole angle\n",
    "# observation[3] = block speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0;\n",
    "    total_time = 100;\n",
    "    for t in range(total_time):\n",
    "        if render: env.render()\n",
    "            \n",
    "        if (observation[2] > 0 and observation[3] > -1) or observation[3] > 1:\n",
    "            action = 1 # right\n",
    "        else:\n",
    "            action = 0 # left\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "\n",
    "    print(\"Total Reward for Episode {} is {} / {}\".format(i_episode, total_reward, total_time))\n",
    "        \n",
    "env.monitor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell was written by Marco Tamassia; All credit to Marco: https://bitbucket.org/marcotamassia/deep-rl\n",
    "# Ported from Python3 using 3to2: 3to2 -w foo.py\n",
    "# Minro edit as marked below.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from itertools import izip\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "class RingBuffer(object):\n",
    "    u\"\"\"\n",
    "    A multi-field ring buffer using numpy arrays.\n",
    "\n",
    "    Adapted from https://scimusing.wordpress.com/2013/10/25/ring-buffers-in-pythonnumpy/\n",
    "    \"\"\"\n",
    "    def __init__(self, memory_size, entries_shape):\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "        self.data = tuple(\n",
    "            np.zeros((memory_size, size), dtype=dtype)\n",
    "            for size, dtype in entries_shape\n",
    "        )\n",
    "        self.max_size = memory_size\n",
    "\n",
    "    def append(self, row):\n",
    "        for data, new_data in izip(self.data, row):\n",
    "            data[self.index, :] = new_data\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def __get__(self, indices):\n",
    "        return tuple(data[(self.index + indices) % self.max_size, :] for data in self.data)\n",
    "\n",
    "    def get_random_entries(self, n):\n",
    "        indices = np.random.randint(0, self.size, n)\n",
    "        return tuple(data[indices, :] for data in self.data)\n",
    "\n",
    "\n",
    "class Experience(object):\n",
    "    u\"\"\"\n",
    "    Experience pool, used to generate batches of random past experience.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, memory_size, discount):\n",
    "        self.discount = discount\n",
    "        self.memory = RingBuffer(\n",
    "            memory_size=memory_size,\n",
    "            entries_shape=(\n",
    "                (state_size, float),\n",
    "                (1, int),\n",
    "                (1, float),\n",
    "                (state_size, float),\n",
    "                (1, bool)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, game_over):\n",
    "        self.memory.append((state, action, reward, new_state, game_over))\n",
    "\n",
    "    def get_batch(self, model, batch_size):\n",
    "        n_rows = min(self.memory.size, batch_size)\n",
    "\n",
    "        S, A, R, NS, GO = self.memory.get_random_entries(n_rows)\n",
    "        A, R, NGO = A.flatten(), R.flatten(), ~GO.flatten()\n",
    "        inputs = S\n",
    "        targets = model.predict(S)\n",
    "        targets[np.arange(len(A)), A] = R\n",
    "        targets[np.where(NGO), A[NGO]] += self.discount * np.max(model.predict(NS[NGO,:]),axis=1)\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def make_net(num_actions, state_size, hidden_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(state_size,)))\n",
    "    model.add(Activation(u'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_actions))\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) # Added\n",
    "    model.compile(optimizer=sgd, loss=u\"MSE\")\n",
    "    return model, sgd # Added sgd\n",
    "\n",
    "\n",
    "def do_the_thing():\n",
    "    episodes = 500 # Changed from 1000 to 500\n",
    "    batch_size = 100\n",
    "    epsilon = 0.1\n",
    "    epsilon_decay = 1e-03\n",
    "    hidden_size = 1000\n",
    "    experience_pool_size = 3000\n",
    "    discount = 0.99\n",
    "\n",
    "    env = gym.make(u\"CartPole-v0\")\n",
    "    exp = Experience(env.state.size, experience_pool_size, discount)\n",
    "    model, sgd = make_net(\n",
    "        num_actions=env.action_space.n,\n",
    "        state_size=env.state.size,\n",
    "        hidden_size=hidden_size\n",
    "    )\n",
    "\n",
    "    for ep_n in xrange(episodes):\n",
    "        state = env.reset()\n",
    "        game_over = False\n",
    "        loss, ret, steps = 0.0, 0, 0\n",
    "        ret = 0\n",
    "        while not game_over:\n",
    "            env.render()\n",
    "\n",
    "            # Choose an epsilon-greedy action\n",
    "            if random.random() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # print(model.predict(state[np.newaxis,:])[0])\n",
    "                action = np.argmax(model.predict(state[np.newaxis,:])[0])\n",
    "            epsilon *= (1 - epsilon_decay)\n",
    "\n",
    "            # Collect experience\n",
    "            new_state, reward, game_over, info = env.step(action)\n",
    "            reward = -1 if game_over else 1 - abs(new_state[2])*10\n",
    "            exp.remember(state, action, reward, new_state, game_over)\n",
    "            state = new_state\n",
    "\n",
    "            # Train model and update stats\n",
    "            inputs, targets = exp.get_batch(model, batch_size=batch_size)\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            ret += reward\n",
    "            steps += 1\n",
    "        print u\"Episode {:03d}/{:03d} | Loss {:.3f} | Return {:.3f} | Steps {:d}\".format(ep_n+1, episodes, loss, ret, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_the_thing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
